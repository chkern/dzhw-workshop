---
title: "Machine Learning Example"
author: "Christoph Kern"
output: html_notebook
---

## Setup

```{r}
library(tidyverse)
library(caret)
library(rpart)
library(ranger)
library(pROC)
```

## Data

For this example, we use the census income data set from the UCI ML repository. It contains "a set of reasonably clean records" from the 1994 Census database. The prediction task is to determine whether a person makes over 50K a year.

Source: https://archive.ics.uci.edu/ml/datasets/Census+Income

First, we load the data and assign variable names.

```{r}
census <- read.csv("census.data", header = FALSE, na.strings = " ?")
varnames <- read.delim("census.names", header = FALSE, skip = 95)
names(census) <- as.character(varnames$V1)
```

Next, we have to clean the factor levels.

```{r}
cln_levels <- function(x){
  levels(x) <- make.names(gsub(" ", "", levels(x)))
  x
}
census[, c(2,4,6,7,8,9,14)] <- lapply(census[, c(2,4,6,7,8,9,14)], cln_levels)
```

In addition, we drop cases with missing values and empty factor levels.

```{r}
census$capital_gain[census$capital_gain >= 99990] <- NA
census <- drop_na(census)
census <- droplevels(census)
```

We also exclude some variables that we won't use in our models.

```{r}
census$fnlwgt <- NULL # Survey weight
census$education <- NULL # We have education_num
census$native_country <- NULL # Rare categories
```

Here we rename the factor levels of the outcome variable and print the frequencies of the outcome categories.

```{r}
levels(census$inc) <- c("under_50K", "over_50K")
summary(census$inc)
```

## Train and test set

Next, we want to split the data into a training (80%) and a test (20%) set. We use `createDataPartition()` from `caret` for this task, which samples within the levels of the outcome variable when splitting the data (i.e. creates stratified splits). 

```{r}
set.seed(92385)
inTrain <- createDataPartition(census$inc, 
                               p = .8, 
                               list = FALSE, 
                               times = 1)
census_train <- census[inTrain,]
census_test <- census[-inTrain,]
```

## CART

With the caret package, we first prepare a helper object via `trainControl()`, which allows us to select the desired evaluation method.

```{r}
ctrl <- trainControl(method = "cv",
                      number = 10,
                      summaryFunction = twoClassSummary,
                      verboseIter = TRUE,
                      classProbs = TRUE)
```

We can now train and tune decision trees with `train()`.

```{r}
cart <- train(inc ~ .,
              data = census_train,
              method = "rpart2",
              trControl = ctrl,
              metric = "ROC")
```

Print the tuning results.

```{r}
cart
```

## Random Forests

Next, we use the `caret` package for training random forests. For this, we specify our own tuning grid. We consider two settings for `mtry` and specify the tree building methods via `splitrule`.

```{r}
grid <- expand.grid(mtry = c(round(sqrt(ncol(census_train))),
                             round(log(ncol(census_train)))),
                    splitrule = c("gini", "extratrees"),
                    min.node.size = 10)
grid
```

Start the tuning process.

```{r}
set.seed(87543)
rf <- train(inc ~ .,
            data = census_train,
            method = "ranger",
            trControl = ctrl,
            tuneGrid = grid,
            metric = "ROC")
```

List the tuning results.

```{r}
rf
```

## Prediction performance

In order to evaluate the performance of the (best) decision tree and random forest, we first compute predicted probabilities and class predictions in the test set.

```{r}
cart_p <- predict(cart, newdata = census_test, type = "prob")
cart_c <- predict(cart, newdata = census_test)

rf_p <- predict(rf, newdata = census_test, type = "prob")
rf_c <- predict(rf, newdata = census_test)
```

The `roc()` function allows us to compute ROC curves and ROC-AUCs.

```{r}
cart_roc <- roc(census_test$inc, cart_p$over_50K)
cart_roc
rf_roc <- roc(census_test$inc, rf_p$over_50K)
rf_roc
```

The resulting object can be plotted with `ggroc()`.

```{r}
ggroc(list(CART = cart_roc, 
           RF = rf_roc)) +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), 
               color="darkgrey", linetype="dashed") +
  theme(legend.title = element_blank())
```

In addition, we can inspect the prediction performance using `confusionMatrix()`, which can be used to display a lot of performance measures, given predicted classes.

```{r}
confusionMatrix(cart_c, census_test$inc, mode = "everything", positive = "over_50K")

confusionMatrix(rf_c, census_test$inc, mode = "everything", positive = "over_50K")
```
